{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Here is the modified code from the original make_dataset.py\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pltpipn\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_train = VOCDetection(root=\"data\", year=\"2007\", image_set=\"train\", download=True)\n",
    "voc_valid = VOCDetection(root=\"data\", year=\"2007\", image_set=\"val\", download=True)\n",
    "voc_test = VOCDetection(root=\"data\", year=\"2007\", image_set=\"test\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVOCDetection(Dataset):\n",
    "    def __init__(self, voc_dataset, transforms=None):\n",
    "        self.voc_dataset = voc_dataset\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.voc_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.voc_dataset[idx]\n",
    "        boxes = target[\"annotation\"][\"object\"]\n",
    "        labels = []\n",
    "        bboxes = []\n",
    "        for box in boxes:\n",
    "            xmin = float(box[\"bndbox\"][\"xmin\"])\n",
    "            ymin = float(box[\"bndbox\"][\"ymin\"])\n",
    "            xmax = float(box[\"bndbox\"][\"xmax\"])\n",
    "            ymax = float(box[\"bndbox\"][\"ymax\"])\n",
    "            bboxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(box['name'])  # assuming all objects are of the same class\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=img, bboxes=bboxes, labels=labels)\n",
    "            img = transformed[\"image\"]\n",
    "            bboxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"labels\"]\n",
    "\n",
    "        return img, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize(img, bboxes):\n",
    "    _, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    for bbox in bboxes:\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "dataset = CustomVOCDetection(voc_train)\n",
    "img, bboxes, labels = dataset[0] \n",
    "\n",
    "# Convert the PIL image to numpy array\n",
    "img_np = np.array(img) \n",
    "\n",
    "# This is not a tensor yet\n",
    "# The shape is (H, W, C)\n",
    "print(img_np.shape)\n",
    "print(type(img_np))\n",
    "visualize(img_np, bboxes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\V: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\i: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\s: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\D: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\r: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\o: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\n: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\e: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\2: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\0: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\1: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\9: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\-: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\D: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\E: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\T: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\-: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\t: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\e: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\s: 0it [00:00, ?it/s]\n",
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\t: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.general import os, Path\n",
    "\n",
    "\n",
    "def visdrone2yolo(dir):\n",
    "    from PIL import Image\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    def convert_box(size, box):\n",
    "        # Convert VisDrone box to YOLO xywh box\n",
    "        dw = 1. / size[0]\n",
    "        dh = 1. / size[1]\n",
    "        return (box[0] + box[2] / 2) * dw, (box[1] + box[3] / 2) * dh, box[2] * dw, box[3] * dh\n",
    "\n",
    "    (dir / 'labels').mkdir(parents=True, exist_ok=True)  # make labels directory\n",
    "    pbar = tqdm((dir / 'annotations').glob('*.txt'), desc=f'Converting {dir}')\n",
    "    for f in pbar:\n",
    "        img_size = Image.open((dir / 'images' / f.name).with_suffix('.jpg')).size\n",
    "        lines = []\n",
    "        with open(f, 'r') as file:  # read annotation.txt\n",
    "            for row in [x.split(',') for x in file.read().strip().splitlines()]:\n",
    "                if row[4] == '0':  # VisDrone 'ignored regions' class 0\n",
    "                    continue\n",
    "                cls = int(row[5]) - 1  #class_num - 1\n",
    "                box = convert_box(img_size, tuple(map(int, row[:4])))\n",
    "                lines.append(f\"{cls} {' '.join(f'{x:.6f}' for x in box)}\\n\")\n",
    "                with open(str(f).replace(os.sep + 'annotations' + os.sep, os.sep + 'labels' + os.sep), 'w') as fl:\n",
    "                    fl.writelines(lines)  # write label.txt\n",
    "\n",
    "\n",
    "dir = Path('C:/Users/ra78lof/yolov5/Mydataset')  # Update this line to your dataset directory if you want to use this code snippet\n",
    "\n",
    "# Convert\n",
    "# C:\\Users\\ra78lof\\yolov5\\Mydataset\\VisDrone2019-VID-test-dev\n",
    "# VisDrone2019-DET-train', 'VisDrone2019-DET-val', \n",
    "# C:\\Users\\ra78lof\\yolov5\\Mydataset\\VisDrone2019-DET-test\n",
    "for d in 'VisDrone2019-DET-test': \n",
    "    visdrone2yolo(dir / d)  # convert VisDrone annotations to YOLO labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting C:\\Users\\ra78lof\\yolov5\\Mydataset\\VisDrone2019-DET-test-dev: 1610it [00:35, 45.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "visdrone2yolo(Path('C:/Users/ra78lof/yolov5/Mydataset/VisDrone2019-DET-test-dev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train: C:/Users/ra78lof/yolov5/Mydataset/VisDrone2019-DET-train/images  # train images (relative to 'path')  6471 images\n",
    "val: C:/Users/ra78lof/yolov5/Mydataset/VisDrone2019-DET-val/images  # val images (relative to 'path')  548 images\n",
    "test: C:/Users/ra78lof/yolov5/Mydataset/VisDrone2019-DET-test-dev/images   # test images (optional)  1610 images\n",
    "\n",
    "# Classes\n",
    "nc: 10  # number of classes\n",
    "names: [ 'pedestrian', 'people', 'bicycle', 'car', 'van', 'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
